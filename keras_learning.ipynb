{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO0oJl9K/2FwjRW1SpxpDZj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/franciskyalo/CARET_PACKAGE/blob/main/keras_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aFq7CvwcXpvA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ACTIVATION FUNCTIONS\n",
        "\n",
        "Activation functions are important because they introduce non-linearity in neural networks enabling them to learn complex patterns that are not linear.\n",
        "\n",
        "\n",
        "* sigmoid function - 0 and 1\n",
        "* hyperbolic tangent - -1, 1\n",
        "* Rectified Linear Unit (ReLu) - Replace negative values with zero aleviating the problem of vanishing gradient descent suffered by sigmoid and Hyperbolic tangent activation functions\n",
        "* Leaky ReLu - Leaky ReLU is an extension of ReLU that allows a small, non-zero gradient when the input is negative, which helps address the dying ReLU problem where neurons can become inactive during training.\n",
        "* Parametric ReLU (PReLU): PReLU is a variant of Leaky ReLU where the slope of the negative part is learned during training instead of being fixed.\n",
        "\n",
        "* Exponential Linear Unit (ELU): ELU function is defined as\n",
        "tends to converge faster than ReLU and can produce negative outputs, which helps prevent dead neurons.\n",
        "* Scaled Exponential Linear Unit (SELU): SELU is a variant of ELU that is designed to self-normalize the activations of each layer, leading to better convergence properties and improved performance for deep neural networks.\n",
        "\n",
        "* Softmax: Softmax function is typically used in the output layer of a neural network for multi-class classification problems. It normalizes the output values into a probability distribution, where each output represents the probability of belonging to a particular class."
      ],
      "metadata": {
        "id": "dqOjxq4leikO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LOSS FUNCTIONS"
      ],
      "metadata": {
        "id": "i474xaSIlxPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss functions are a critical component of deep learning models, serving as a measure of how well the model's predictions match the actual target values. The choice of a loss function depends on the specific task being addressed (e.g., classification, regression, generative modeling) and the characteristics of the data. In this discussion, I'll cover several common loss functions used in deep learning and their applications.\n",
        "\n",
        "### Classification Loss Functions:\n",
        "\n",
        "1. **Binary Cross-Entropy Loss (Log Loss)**:\n",
        "   - This loss function is commonly used for binary classification tasks, where the target variable has two classes (e.g., 0 or 1).\n",
        "   - It measures the difference between the predicted probabilities and the true binary labels.\n",
        "   - The formula for binary cross-entropy loss is:\n",
        "     \\[ \\text{Binary Cross-Entropy Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} (y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i)) \\]\n",
        "     where \\( N \\) is the number of samples, \\( y_i \\) is the true label (0 or 1), and \\( \\hat{y}_i \\) is the predicted probability of class 1.\n",
        "\n",
        "2. **Categorical Cross-Entropy Loss**:\n",
        "   - Categorical cross-entropy loss is used for multi-class classification tasks, where the target variable has more than two classes.\n",
        "   - It compares the predicted class probabilities with the true one-hot encoded labels.\n",
        "   - The formula for categorical cross-entropy loss is:\n",
        "     \\[ \\text{Categorical Cross-Entropy Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{ij} \\log(\\hat{y}_{ij}) \\]\n",
        "     where \\( C \\) is the number of classes, \\( y_{ij} \\) is 1 if sample \\( i \\) belongs to class \\( j \\) and 0 otherwise, and \\( \\hat{y}_{ij} \\) is the predicted probability of sample \\( i \\) belonging to class \\( j \\).\n",
        "\n",
        "3. **Sparse Categorical Cross-Entropy Loss**:\n",
        "   - Similar to categorical cross-entropy loss but used when the target labels are integers instead of one-hot encoded vectors.\n",
        "   - It is more memory efficient than categorical cross-entropy when dealing with a large number of classes.\n",
        "\n",
        "### Regression Loss Functions:\n",
        "\n",
        "1. **Mean Squared Error (MSE) Loss**:\n",
        "   - MSE loss is widely used for regression tasks, where the target variable is continuous.\n",
        "   - It measures the average squared difference between the predicted values and the true target values.\n",
        "   - The formula for MSE loss is:\n",
        "     \\[ \\text{MSE Loss} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 \\]\n",
        "     where \\( N \\) is the number of samples, \\( y_i \\) is the true target value, and \\( \\hat{y}_i \\) is the predicted value.\n",
        "\n",
        "2. **Mean Absolute Error (MAE) Loss**:\n",
        "   - MAE loss is another regression loss function that measures the average absolute difference between the predicted values and the true target values.\n",
        "   - It is less sensitive to outliers compared to MSE loss.\n",
        "   - The formula for MAE loss is:\n",
        "     \\[ \\text{MAE Loss} = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i| \\]\n",
        "\n",
        "### Other Loss Functions:\n",
        "\n",
        "1. **Huber Loss**:\n",
        "   - Huber loss combines the best properties of MSE and MAE loss functions.\n",
        "   - It is less sensitive to outliers like MAE but provides the gradient properties of MSE loss near the minimum.\n",
        "   - Huber loss switches between L2 and L1 loss depending on the error magnitude.\n",
        "   - It is particularly useful when dealing with noisy data or data with outliers.\n",
        "\n",
        "2. **Kullback-Leibler Divergence (KL Divergence)**:\n",
        "   - KL divergence measures the difference between two probability distributions.\n",
        "   - It is commonly used in variational autoencoders (VAEs) and other generative models as a regularization term.\n",
        "   - In classification tasks, it can be used as a regularization term to penalize differences between the predicted and target distributions.\n",
        "\n",
        "3. **Dice Loss**:\n",
        "   - Dice loss is commonly used in semantic segmentation tasks.\n",
        "   - It measures the overlap between the predicted segmentation masks and the ground truth masks.\n",
        "   - Dice loss encourages better localization of object boundaries and is robust to class imbalance.\n",
        "\n",
        "4. **Focal Loss**:\n",
        "   - Focal loss is used to address class imbalance in classification tasks.\n",
        "   - It down-weights the loss assigned to well-classified examples, focusing more on hard-to-classify examples.\n",
        "   - Focal loss helps in improving the performance of the model on minority classes while mitigating the effect of dominant classes.\n",
        "\n",
        "5. **Contrastive Loss**:\n",
        "   - Contrastive loss is used in siamese networks and metric learning tasks.\n",
        "   - It encourages similar samples to be closer together in the embedding space while pushing dissimilar samples apart.\n",
        "   - Contrastive loss is used for learning similarity metrics and embeddings.\n",
        "\n",
        "These are some of the commonly used loss functions in deep learning. The choice of a loss function depends on factors such as the nature of the task, the characteristics of the data, and the desired properties of the model's predictions. Experimentation with different loss functions is often necessary to find the one that best suits the specific requirements of a given task."
      ],
      "metadata": {
        "id": "0gXoZE34l9a4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPTIMIZATION TECHNIQUES\n",
        "\n",
        "There are various optimization techniques used in deep learning to update the parameters (weights and biases) of neural networks during training in order to minimize the loss function. The choice of optimization technique depends on factors such as the network architecture, the size of the dataset, computational resources, and the specific characteristics of the problem being solved. Here are some common optimization techniques in deep learning and when they are best used:\n",
        "\n",
        "1. **Stochastic Gradient Descent (SGD)**:\n",
        "   - SGD is the simplest optimization algorithm used in deep learning.\n",
        "   - It updates the model parameters based on the gradient of the loss function computed on a mini-batch of training data.\n",
        "   - SGD is best used when working with large datasets or when computational resources are limited.\n",
        "   - However, it can be slow to converge and may suffer from oscillations, especially in the presence of noise.\n",
        "\n",
        "2. **Mini-Batch Gradient Descent**:\n",
        "   - Mini-batch gradient descent is a variation of SGD where the gradient is computed on a mini-batch of training data instead of the entire dataset.\n",
        "   - It strikes a balance between the computational efficiency of SGD and the stability of batch gradient descent.\n",
        "   - Mini-batch gradient descent is commonly used in practice for training deep neural networks.\n",
        "\n",
        "3. **Momentum**:\n",
        "   - Momentum is a technique that accelerates SGD by accumulating a momentum term that determines the direction of the update.\n",
        "   - It helps overcome the oscillations and slow convergence of SGD by adding inertia to the parameter updates.\n",
        "   - Momentum is particularly useful when dealing with high-dimensional optimization problems or noisy gradients.\n",
        "\n",
        "4. **Adaptive Learning Rate Methods**:\n",
        "   - Adaptive learning rate methods dynamically adjust the learning rate during training based on the past gradients.\n",
        "   - Examples include AdaGrad, RMSprop, and Adam.\n",
        "   - These methods are well-suited for non-convex optimization problems with sparse gradients or varying curvature.\n",
        "   - Adam, in particular, is widely used in practice due to its robustness and effectiveness across a wide range of tasks.\n",
        "\n",
        "5. **Learning Rate Scheduling**:\n",
        "   - Learning rate scheduling involves systematically reducing the learning rate over the course of training.\n",
        "   - It helps stabilize training and improve convergence by gradually reducing the step size as the optimization process progresses.\n",
        "   - Learning rate scheduling is commonly used in conjunction with other optimization techniques, such as SGD or Adam.\n",
        "\n",
        "6. **Nesterov Accelerated Gradient (NAG)**:\n",
        "   - Nesterov Accelerated Gradient is a variant of momentum optimization that improves convergence by using a \"lookahead\" update.\n",
        "   - It computes the gradient at a future position based on the current momentum direction before updating the parameters.\n",
        "   - NAG is effective in speeding up convergence and reducing oscillations in training.\n",
        "\n",
        "7. **Adaptive Moment Estimation (AdamW)**:\n",
        "   - AdamW is a variant of Adam that includes weight decay regularization to prevent overfitting.\n",
        "   - It decouples weight decay from the optimization variables, resulting in improved generalization performance.\n",
        "   - AdamW is particularly useful when training deep neural networks with large amounts of data and complex architectures.\n",
        "\n",
        "8. **Second-Order Optimization Methods**:\n",
        "   - Second-order optimization methods, such as Newton's method and conjugate gradient descent, use second-order information (e.g., Hessian matrix) to update the parameters.\n",
        "   - These methods can converge faster than first-order methods but are computationally expensive and impractical for large-scale deep learning tasks.\n",
        "\n",
        "The choice of optimization technique depends on factors such as the complexity of the optimization problem, the size of the dataset, the computational resources available, and the desired convergence properties. Experimentation and tuning are often necessary to find the optimal optimization strategy for a given deep learning task.\n"
      ],
      "metadata": {
        "id": "hKz6LzWqnZjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BACK PROPERGATION\n",
        "\n",
        "Backpropagation, short for \"backward propagation of errors,\" is a fundamental algorithm used to train neural networks in deep learning. It's crucial for updating the parameters (weights and biases) of the network in a way that minimizes the difference between the model's predictions and the actual target values. Here's an in-depth discussion of backpropagation and its importance in deep learning:\n",
        "\n",
        "### 1. Forward Pass:\n",
        "   - During the forward pass, the input data is fed into the neural network, and the activations of each layer are computed sequentially.\n",
        "   - The activations are computed by applying the activation function to the weighted sum of inputs to each neuron.\n",
        "\n",
        "### 2. Loss Calculation:\n",
        "   - Once the forward pass is complete, the loss function is computed to measure the discrepancy between the model's predictions and the true target values.\n",
        "   - The choice of loss function depends on the task being performed (e.g., classification, regression).\n",
        "\n",
        "### 3. Backward Pass (Backpropagation):\n",
        "   - In the backward pass, the gradient of the loss function with respect to each parameter in the network is computed.\n",
        "   - Backpropagation involves propagating this gradient backward through the network, layer by layer, using the chain rule of calculus.\n",
        "\n",
        "### 4. Chain Rule:\n",
        "   - The chain rule allows us to decompose the gradient of the loss function with respect to the parameters of the network into a series of simpler gradients, which can be computed efficiently.\n",
        "   - It enables us to compute the gradient of the loss function with respect to the parameters of each layer in the network.\n",
        "\n",
        "### 5. Parameter Updates:\n",
        "   - Once the gradients have been computed for all parameters in the network, the parameters are updated using an optimization algorithm such as stochastic gradient descent (SGD) or Adam.\n",
        "   - The magnitude and direction of the updates are determined by the gradients and the learning rate.\n",
        "\n",
        "### Importance of Backpropagation in Deep Learning:\n",
        "\n",
        "1. **Efficient Training**:\n",
        "   - Backpropagation allows neural networks to efficiently learn from data by iteratively updating their parameters to minimize the loss function.\n",
        "   - Without backpropagation, training deep neural networks would be computationally infeasible due to the sheer number of parameters involved.\n",
        "\n",
        "2. **Automatic Differentiation**:\n",
        "   - Backpropagation automates the process of computing gradients, freeing practitioners from manually deriving and implementing complex derivatives.\n",
        "   - It enables the use of gradient-based optimization algorithms to train deep neural networks effectively.\n",
        "\n",
        "3. **Learning Complex Representations**:\n",
        "   - Backpropagation enables deep neural networks to learn complex hierarchical representations of the input data.\n",
        "   - By propagating gradients through multiple layers, neural networks can capture intricate patterns and relationships in the data, leading to improved performance on various tasks.\n",
        "\n",
        "4. **Scalability**:\n",
        "   - Backpropagation scales to large datasets and deep network architectures, making it suitable for training modern deep learning models on vast amounts of data.\n",
        "   - It allows researchers and practitioners to tackle a wide range of real-world problems across different domains.\n",
        "\n",
        "5. **Generalization**:\n",
        "   - Through backpropagation, neural networks learn to generalize from training data to unseen examples, leading to models that exhibit robust performance on new data.\n",
        "   - The iterative nature of backpropagation allows neural networks to iteratively refine their parameters, improving generalization performance over time.\n",
        "\n",
        "In summary, backpropagation is a fundamental algorithm in deep learning that enables efficient training of neural networks by computing gradients and updating parameters iteratively. Its importance lies in its ability to automate the process of gradient computation, allowing neural networks to learn complex representations from data and generalize well to unseen examples."
      ],
      "metadata": {
        "id": "mUOud6zIpDmG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REGULARIZATION TECHNIQUES\n",
        "Regularization techniques are crucial in deep learning for preventing overfitting, improving the generalization performance of models, and enhancing their robustness to unseen data. Overfitting occurs when a model learns to memorize the training data rather than capturing the underlying patterns in the data, leading to poor performance on new, unseen examples. Regularization techniques introduce constraints or penalties to the optimization process, encouraging the model to learn simpler and more generalizable representations. Here's an in-depth discussion of various regularization techniques used in deep learning:\n",
        "\n",
        "### 1. L1 and L2 Regularization (Weight Decay):\n",
        "   - **L1 regularization**: Adds a penalty term to the loss function proportional to the absolute values of the weights.\n",
        "     \\[ \\text{L1 Loss} = \\lambda \\sum_{i=1}^{n} |w_i| \\]\n",
        "   - **L2 regularization**: Adds a penalty term to the loss function proportional to the squared magnitudes of the weights.\n",
        "     \\[ \\text{L2 Loss} = \\lambda \\sum_{i=1}^{n} w_i^2 \\]\n",
        "   - \\( \\lambda \\) is the regularization strength hyperparameter.\n",
        "   - L1 regularization encourages sparsity in the weights, making some of them exactly zero, leading to feature selection.\n",
        "   - L2 regularization, also known as weight decay, penalizes large weights and helps prevent them from growing too large, effectively reducing model complexity.\n",
        "\n",
        "### 2. Dropout:\n",
        "   - Dropout is a regularization technique that randomly drops a fraction of neurons (along with their connections) from the network during training.\n",
        "   - It prevents co-adaptation of neurons and encourages the network to learn more robust and distributed representations.\n",
        "   - Dropout effectively simulates an ensemble of smaller networks, leading to improved generalization performance.\n",
        "   - At test time, dropout is usually turned off or scaled down to account for the dropped neurons during training.\n",
        "\n",
        "### 3. Batch Normalization:\n",
        "   - Batch normalization is a technique that normalizes the activations of each layer across mini-batches during training.\n",
        "   - It helps stabilize and speed up training by reducing internal covariate shift.\n",
        "   - Batch normalization acts as a regularizer by introducing noise to the activations, similar to dropout.\n",
        "   - It allows for the use of higher learning rates and helps prevent the vanishing or exploding gradient problem.\n",
        "\n",
        "### 4. Early Stopping:\n",
        "   - Early stopping is a simple regularization technique that stops training the model when the performance on a validation set starts to degrade.\n",
        "   - It prevents the model from overfitting by terminating training before it becomes too specialized to the training data.\n",
        "   - Early stopping requires monitoring the validation performance during training and saving the model parameters when the performance is optimal.\n",
        "\n",
        "### 5. Data Augmentation:\n",
        "   - Data augmentation is a technique used to increase the size and diversity of the training data by applying transformations such as rotation, translation, scaling, and flipping.\n",
        "   - It helps the model generalize better by exposing it to a wider range of variations and patterns in the data.\n",
        "   - Data augmentation is particularly useful when working with limited training data or when dealing with imbalanced datasets.\n",
        "\n",
        "### 6. DropConnect:\n",
        "   - DropConnect is an extension of dropout that randomly sets a fraction of weights to zero instead of neurons during training.\n",
        "   - It regularizes the model by preventing co-adaptation of weights, similar to dropout.\n",
        "   - DropConnect can be applied to both fully connected and convolutional layers, providing a flexible regularization mechanism.\n",
        "\n",
        "### 7. Mixup and CutMix:\n",
        "   - Mixup and CutMix are data augmentation techniques that combine pairs of training samples or patches of images, respectively, by mixing their features and labels.\n",
        "   - They encourage the model to learn more robust decision boundaries and reduce the risk of overfitting.\n",
        "   - Mixup and CutMix are particularly effective in image classification tasks and can improve the generalization performance of deep learning models.\n",
        "\n",
        "### 8. Weight Constraint:\n",
        "   - Weight constraint regularization imposes a constraint on the magnitude of the weights during optimization.\n",
        "   - It can be implemented by clipping the weights to a maximum value or by applying a penalty to the norm of the weights.\n",
        "   - Weight constraint regularization helps prevent the model from learning overly complex representations and encourages more stable optimization.\n",
        "\n",
        "### 9. Ensemble Methods:\n",
        "   - Ensemble methods combine predictions from multiple models to make a final prediction, reducing the risk of overfitting and improving generalization performance.\n",
        "   - Bagging (Bootstrap Aggregating), boosting, and stacking are common ensemble methods used in deep learning.\n",
        "   - Ensemble methods are effective in reducing model variance and achieving higher predictive accuracy on diverse datasets.\n",
        "\n",
        "Regularization techniques play a crucial role in training deep learning models by preventing overfitting, improving generalization performance, and enhancing model robustness. The choice of regularization technique depends on factors such as the complexity of the model, the size and diversity of the training data, and the specific characteristics of the problem being solved. Experimentation and tuning are often necessary to determine the optimal combination of regularization techniques for a given deep learning task."
      ],
      "metadata": {
        "id": "BhpGCZ4_qJ40"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEEP LEARNING ARCHITECTURES\n",
        "\n",
        "Deep learning architectures are specialized neural network structures designed to solve specific types of problems or handle particular types of data. Each architecture has its own characteristics, advantages, and best use cases. Here's a discussion of some common deep learning architectures and the situations they are best suited for:\n",
        "\n",
        "### 1. Convolutional Neural Networks (CNNs):\n",
        "   - **Best Suited For**: Image and video processing tasks such as image classification, object detection, and image segmentation.\n",
        "   - **Characteristics**:\n",
        "     - CNNs are designed to efficiently handle grid-structured data like images by leveraging shared weights and local connectivity.\n",
        "     - They consist of convolutional layers for feature extraction, pooling layers for spatial downsampling, and fully connected layers for classification.\n",
        "   - **Applications**:\n",
        "     - Image classification (e.g., classifying images into different categories).\n",
        "     - Object detection (e.g., detecting and localizing objects within images).\n",
        "     - Image segmentation (e.g., segmenting images into different regions or objects).\n",
        "\n",
        "### 2. Recurrent Neural Networks (RNNs):\n",
        "   - **Best Suited For**: Sequential data processing tasks such as time series analysis, natural language processing (NLP), and speech recognition.\n",
        "   - **Characteristics**:\n",
        "     - RNNs have recurrent connections that allow them to maintain a memory of past inputs, making them suitable for sequential data processing.\n",
        "     - They can handle inputs of variable length and are well-suited for tasks involving time-series data or sequences of symbols.\n",
        "   - **Applications**:\n",
        "     - Language modeling (e.g., predicting the next word in a sentence).\n",
        "     - Machine translation (e.g., translating text from one language to another).\n",
        "     - Speech recognition (e.g., transcribing spoken language into text).\n",
        "\n",
        "### 3. Long Short-Term Memory Networks (LSTMs) and Gated Recurrent Unit Networks (GRUs):\n",
        "   - **Best Suited For**: Sequential data processing tasks where capturing long-range dependencies is important, such as machine translation, speech recognition, and sentiment analysis.\n",
        "   - **Characteristics**:\n",
        "     - LSTMs and GRUs are variants of RNNs designed to address the vanishing gradient problem and capture long-range dependencies more effectively.\n",
        "     - They use gating mechanisms to control the flow of information through the network and maintain a more stable gradient flow during training.\n",
        "   - **Applications**:\n",
        "     - Sentiment analysis (e.g., classifying the sentiment of text documents).\n",
        "     - Time series forecasting (e.g., predicting future stock prices or weather patterns).\n",
        "     - Music generation (e.g., generating new pieces of music based on existing compositions).\n",
        "\n",
        "### 4. Autoencoders:\n",
        "   - **Best Suited For**: Unsupervised learning tasks such as data denoising, dimensionality reduction, and anomaly detection.\n",
        "   - **Characteristics**:\n",
        "     - Autoencoders are neural networks trained to reconstruct their input data at the output layer, typically through an encoder-decoder architecture.\n",
        "     - They learn a compact representation of the input data by compressing and then reconstructing it, which can be useful for feature extraction or data generation.\n",
        "   - **Applications**:\n",
        "     - Data compression (e.g., reducing the dimensionality of high-dimensional data).\n",
        "     - Anomaly detection (e.g., detecting outliers or anomalies in data streams).\n",
        "     - Image denoising (e.g., removing noise from images while preserving important features).\n",
        "\n",
        "### 5. Generative Adversarial Networks (GANs):\n",
        "   - **Best Suited For**: Generating realistic synthetic data, image-to-image translation, and unsupervised representation learning.\n",
        "   - **Characteristics**:\n",
        "     - GANs consist of two neural networks—a generator and a discriminator—trained simultaneously in a competitive setting.\n",
        "     - The generator learns to generate synthetic data samples that are indistinguishable from real data, while the discriminator learns to differentiate between real and fake samples.\n",
        "   - **Applications**:\n",
        "     - Image generation (e.g., generating realistic images of human faces or animals).\n",
        "     - Style transfer (e.g., converting images from one artistic style to another).\n",
        "     - Semi-supervised learning (e.g., using generated data to augment labeled datasets for improved model performance).\n",
        "\n",
        "### 6. Transformer Networks:\n",
        "   - **Best Suited For**: Natural language processing tasks such as machine translation, language modeling, and text classification.\n",
        "   - **Characteristics**:\n",
        "     - Transformer networks are based on self-attention mechanisms that enable them to capture global dependencies in sequential data more efficiently than recurrent architectures.\n",
        "     - They operate on entire sequences of data in parallel, making them highly scalable and suitable for long-range sequence modeling.\n",
        "   - **Applications**:\n",
        "     - Machine translation (e.g., translating text from one language to another).\n",
        "     - Language modeling (e.g., predicting the next word in a sequence of text).\n",
        "     - Document summarization (e.g., generating concise summaries of long documents).\n",
        "\n",
        "These are some of the most commonly used deep learning architectures and their respective best use cases. It's important to note that the choice of architecture depends on factors such as the nature of the data, the specific task being performed, and the computational resources available. Experimentation and domain expertise are often necessary to select the most appropriate architecture for a given problem."
      ],
      "metadata": {
        "id": "93f3qguduByU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA PREPROCESSING IN DEEP LEARNING\n",
        "\n",
        "Data preprocessing is a crucial step in deep learning that involves preparing and cleaning the raw input data to make it suitable for training deep neural networks. Effective data preprocessing can improve the performance and robustness of deep learning models by reducing noise, handling missing values, and normalizing the data. Here are some common data preprocessing techniques used in deep learning:\n",
        "\n",
        "### 1. Data Cleaning:\n",
        "   - **Handling Missing Values**: Missing values in the dataset can be filled using techniques such as mean, median, or mode imputation, or using more sophisticated methods like interpolation or predictive modeling.\n",
        "   - **Removing Outliers**: Outliers can be identified and removed using statistical methods such as Z-score, IQR (Interquartile Range), or by using domain knowledge.\n",
        "   - **Data Deduplication**: Removing duplicate records from the dataset to ensure that each data point is unique and contributes meaningfully to the training process.\n",
        "\n",
        "### 2. Data Normalization and Scaling:\n",
        "   - **Normalization**: Scaling the features to a similar range, typically between 0 and 1 or -1 and 1. Common normalization techniques include min-max scaling and z-score normalization.\n",
        "   - **Standardization**: Transforming the features to have a mean of 0 and a standard deviation of 1. This ensures that features are centered around zero and have similar scales.\n",
        "   - **Log Transformation**: Applying logarithmic transformation to features with skewed distributions to make them more normally distributed.\n",
        "\n",
        "### 3. Feature Engineering:\n",
        "   - **One-Hot Encoding**: Converting categorical variables into binary vectors, where each category is represented by a binary feature. This allows categorical variables to be included in the model.\n",
        "   - **Feature Scaling**: Scaling numerical features to ensure that they have similar ranges and magnitudes, preventing features with larger scales from dominating the training process.\n",
        "   - **Feature Selection**: Selecting a subset of relevant features that are most informative for the target variable, either through statistical tests, domain knowledge, or feature importance techniques.\n",
        "\n",
        "### 4. Data Augmentation:\n",
        "   - **Image Data Augmentation**: Generating new training samples by applying transformations such as rotation, translation, scaling, flipping, or adding noise to images. This helps in increasing the diversity and size of the training dataset, leading to improved generalization performance.\n",
        "   - **Text Data Augmentation**: Augmenting text data by adding synonyms, paraphrases, or perturbations to existing text samples. This can help in creating a more robust model for tasks such as text classification or sentiment analysis.\n",
        "\n",
        "### 5. Dimensionality Reduction:\n",
        "   - **Principal Component Analysis (PCA)**: A dimensionality reduction technique that identifies the most important features in the dataset and projects the data onto a lower-dimensional space while preserving the variance.\n",
        "   - **t-Distributed Stochastic Neighbor Embedding (t-SNE)**: A non-linear dimensionality reduction technique used for visualizing high-dimensional data in lower-dimensional space while preserving local similarities.\n",
        "\n",
        "### 6. Handling Imbalanced Data:\n",
        "   - **Oversampling**: Increasing the number of minority class samples by generating synthetic samples or duplicating existing ones.\n",
        "   - **Undersampling**: Reducing the number of majority class samples to balance the class distribution.\n",
        "   - **SMOTE (Synthetic Minority Over-sampling Technique)**: A method for synthesizing new minority class samples by interpolating between existing samples.\n",
        "\n",
        "### 7. Time Series Preprocessing:\n",
        "   - **Resampling**: Changing the frequency of the time series data by upsampling (increasing the frequency) or downsampling (decreasing the frequency).\n",
        "   - **Feature Engineering**: Creating lag features, rolling window statistics, or time-based features to capture temporal patterns and trends in the time series data.\n",
        "   - **Normalization**: Scaling the time series data to a similar range to ensure that features have consistent scales.\n",
        "\n",
        "### 8. Handling Text Data:\n",
        "   - **Tokenization**: Splitting text into individual words or tokens for further processing.\n",
        "   - **Text Cleaning**: Removing punctuation, special characters, stopwords, and converting text to lowercase.\n",
        "   - **Word Embeddings**: Representing words as dense vectors in a continuous vector space to capture semantic relationships between words.\n",
        "\n",
        "### 9. Handling Categorical Data:\n",
        "   - **Label Encoding**: Converting categorical variables into ordinal integers.\n",
        "   - **Embedding Layers**: Representing categorical variables as dense vectors in a continuous vector space, typically learned during the training process.\n",
        "\n",
        "### 10. Handling Time-Varying Data:\n",
        "   - **Temporal Aggregation**: Aggregating time-varying data over fixed time intervals (e.g., hourly, daily) to create features for prediction tasks.\n",
        "   - **Windowing**: Creating overlapping or non-overlapping windows of time series data for input into the model.\n",
        "\n",
        "### 11. Handling Spatial Data:\n",
        "   - **Resizing**: Resizing spatial data (e.g., images) to a fixed size to ensure consistency across samples.\n",
        "   - **Normalization**: Scaling pixel values to a similar range (e.g., [0, 1]) to improve convergence during training.\n",
        "\n",
        "These are some common data preprocessing techniques used in deep learning to clean, preprocess, and prepare the input data for training deep neural networks. The choice of preprocessing techniques depends on the nature of the data, the specific task being addressed, and the requirements of the deep learning model. Experimentation and domain expertise are often necessary to determine the most appropriate preprocessing pipeline for a given problem."
      ],
      "metadata": {
        "id": "gZjY4ZnovhKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HYPERPARAMETERS IN DEEP LEARNING\n",
        "\n",
        "Hyperparameters are parameters that are set before the training process begins and govern the architecture and behavior of the neural network during training. Choosing appropriate hyperparameters is crucial for achieving optimal performance and generalization in deep learning models. Here are some common hyperparameters used in deep learning, along with a discussion of their significance:\n",
        "\n",
        "### 1. Learning Rate:\n",
        "   - **Definition**: The learning rate determines the step size of parameter updates during optimization. It controls how much the model parameters are adjusted with respect to the gradient of the loss function.\n",
        "   - **Significance**: The learning rate is one of the most important hyperparameters, as it affects the convergence speed and stability of the training process. A high learning rate may cause the optimization process to overshoot the minimum, while a low learning rate may lead to slow convergence or getting stuck in local minima.\n",
        "\n",
        "### 2. Batch Size:\n",
        "   - **Definition**: The batch size determines the number of samples processed by the model before updating the parameters. It affects the stability of the optimization process and the memory requirements during training.\n",
        "   - **Significance**: Choosing an appropriate batch size balances between computational efficiency and the quality of parameter updates. Larger batch sizes can lead to faster convergence but may require more memory and computational resources.\n",
        "\n",
        "### 3. Number of Epochs:\n",
        "   - **Definition**: An epoch refers to one complete pass through the entire training dataset during the training process. The number of epochs specifies how many times the entire dataset is processed by the model.\n",
        "   - **Significance**: The number of epochs determines the duration of training and affects the convergence and generalization performance of the model. Too few epochs may result in underfitting, while too many epochs may lead to overfitting.\n",
        "\n",
        "### 4. Network Architecture:\n",
        "   - **Definition**: The network architecture refers to the structure and topology of the neural network, including the number of layers, the types of layers (e.g., convolutional, recurrent), and the number of neurons in each layer.\n",
        "   - **Significance**: The network architecture determines the capacity and expressiveness of the model, influencing its ability to learn complex patterns and relationships in the data. Choosing an appropriate architecture depends on the specific task and the characteristics of the dataset.\n",
        "\n",
        "### 5. Activation Functions:\n",
        "   - **Definition**: Activation functions introduce non-linearity to the neural network by transforming the weighted sum of inputs at each neuron into an output signal.\n",
        "   - **Significance**: The choice of activation function affects the model's ability to approximate complex functions and learn non-linear relationships in the data. Common activation functions include ReLU (Rectified Linear Unit), sigmoid, tanh (hyperbolic tangent), and softmax.\n",
        "\n",
        "### 6. Regularization Parameters:\n",
        "   - **Definition**: Regularization parameters control the strength of regularization techniques such as L1 and L2 regularization, dropout, and batch normalization.\n",
        "   - **Significance**: Regularization parameters help prevent overfitting by penalizing overly complex models and promoting simpler, more generalizable solutions. Tuning regularization parameters is essential for achieving the right balance between bias and variance in the model.\n",
        "\n",
        "### 7. Optimizer Parameters:\n",
        "   - **Definition**: Optimizer parameters control the behavior of optimization algorithms such as SGD (Stochastic Gradient Descent), Adam, RMSprop, and AdaGrad.\n",
        "   - **Significance**: Optimizer parameters determine the update rules for model parameters during training, affecting the convergence speed, stability, and performance of the model. Tuning optimizer parameters can significantly impact the training process and the final model performance.\n",
        "\n",
        "### 8. Dropout Rate:\n",
        "   - **Definition**: Dropout is a regularization technique that randomly drops a fraction of neurons (along with their connections) from the network during training.\n",
        "   - **Significance**: The dropout rate determines the probability of dropping neurons during training. Higher dropout rates increase regularization and help prevent overfitting by reducing co-adaptation of neurons.\n",
        "\n",
        "### 9. Weight Initialization:\n",
        "   - **Definition**: Weight initialization specifies the initial values of the model parameters (weights and biases) before training begins.\n",
        "   - **Significance**: Proper weight initialization is critical for preventing vanishing or exploding gradients and ensuring stable training. Common weight initialization techniques include random initialization, Xavier initialization, and He initialization.\n",
        "\n",
        "### 10. Learning Rate Schedule:\n",
        "   - **Definition**: Learning rate schedule specifies how the learning rate changes over time during training. It can be constant, decayed, or adaptive based on various factors.\n",
        "   - **Significance**: Learning rate schedule can help improve the convergence and generalization performance of the model by adjusting the learning rate dynamically during training. Common learning rate schedules include step decay, exponential decay, and cyclical learning rates.\n",
        "\n",
        "Choosing appropriate hyperparameters requires a combination of domain knowledge, experimentation, and hyperparameter tuning techniques such as grid search, random search, or Bayesian optimization. Fine-tuning hyperparameters is an iterative process that involves training multiple models with different hyperparameter configurations and evaluating their performance on a validation set to identify the optimal settings for the specific task and dataset."
      ],
      "metadata": {
        "id": "CpdpfLevwBcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AN INTRODUCTION TO KERAS API"
      ],
      "metadata": {
        "id": "hzoKWBMt1Lx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " import numpy as np\n",
        " from random import randint\n",
        " from sklearn.utils import shuffle\n",
        " from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "gQ9McAn61QMm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = []\n",
        "train_samples = []"
      ],
      "metadata": {
        "id": "jHgM27U84Oeg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(50):\n",
        "  random_younger = randint(13,64)\n",
        "  train_samples.append(random_younger)\n",
        "  train_labels.append(1)\n",
        "\n",
        "  random_older = randint(65,100)\n",
        "  train_samples.append(random_older)\n",
        "  train_labels.append(0)\n",
        "\n",
        "for i in range(1000):\n",
        "  random_younger = randint(13,64)\n",
        "  train_samples.append(random_younger)\n",
        "  train_labels.append(1)\n",
        "\n",
        "  random_older = randint(65,100)\n",
        "  train_samples.append(random_older)\n",
        "  train_labels.append(0)"
      ],
      "metadata": {
        "id": "Xf0XWekw4ZBK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame({\"samples\":train_samples,\n",
        "                     \"labels\":train_labels})"
      ],
      "metadata": {
        "id": "3RYJ8mdy4shk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "nodFgps-6ZtV",
        "outputId": "313a0816-3613-4d8f-c0fa-14dfce63e9d2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   samples  labels\n",
              "0       52       1\n",
              "1       80       0\n",
              "2       37       1\n",
              "3       95       0\n",
              "4       14       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1cdaea74-38ef-47e0-b581-7924301b3eb2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>samples</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>37</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>95</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1cdaea74-38ef-47e0-b581-7924301b3eb2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1cdaea74-38ef-47e0-b581-7924301b3eb2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1cdaea74-38ef-47e0-b581-7924301b3eb2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ec5476d7-3885-446d-8828-8c6aa9c9051c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ec5476d7-3885-446d-8828-8c6aa9c9051c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ec5476d7-3885-446d-8828-8c6aa9c9051c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 2100,\n  \"fields\": [\n    {\n      \"column\": \"samples\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 25,\n        \"min\": 13,\n        \"max\": 100,\n        \"samples\": [\n          58,\n          52,\n          72\n        ],\n        \"num_unique_values\": 88,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"num_unique_values\": 2,\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = np.array(train_labels)\n",
        "train_samples = np.array(train_samples)\n",
        "\n",
        "train_labels, train_samples = shuffle(train_labels, train_samples)"
      ],
      "metadata": {
        "id": "_gPNBmBA6f9x"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler(feature_range =(0,1))\n",
        "\n",
        "scaled_train_samples = scaler.fit_transform(train_samples.reshape(-1,1))"
      ],
      "metadata": {
        "id": "UwaEfnd36_U9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in scaled_train_samples[0:10]:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wygdcdpu8yQt",
        "outputId": "8456febe-e644-43ca-f2f9-3aa561a4cf13"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.27586207]\n",
            "[0.91954023]\n",
            "[0.96551724]\n",
            "[0.]\n",
            "[0.32183908]\n",
            "[0.59770115]\n",
            "[0.79310345]\n",
            "[0.04597701]\n",
            "[0.3908046]\n",
            "[0.98850575]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple tf.keras sequntial model with data"
      ],
      "metadata": {
        "id": "2JWgvpAZ9IcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using a sequential model using the Keras API\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import categorical_crossentropy\n",
        "\n"
      ],
      "metadata": {
        "id": "P5uNThy888oj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Dense(units=16, input_shape=(1,), activation='relu'),\n",
        "    Dense(units=32, activation='relu'),\n",
        "    Dense(units=2, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "mjUNZpqR-T_h"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_FCR3Bb_FmA",
        "outputId": "c4c43cd8-8336-4b92-be2a-209835659f02"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 16)                32        \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                544       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 642 (2.51 KB)\n",
            "Trainable params: 642 (2.51 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "5P4XofVk_PhN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x=scaled_train_samples, y= train_labels, validation_split=0.1,batch_size=10, epochs=30, shuffle=True, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66_BdgKrBCY9",
        "outputId": "8a2755a3-d5cb-41be-de57-83dd1760b432"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "189/189 - 1s - loss: 0.6628 - accuracy: 0.5730 - val_loss: 0.6506 - val_accuracy: 0.6000 - 1s/epoch - 7ms/step\n",
            "Epoch 2/30\n",
            "189/189 - 0s - loss: 0.6282 - accuracy: 0.6772 - val_loss: 0.6198 - val_accuracy: 0.6810 - 323ms/epoch - 2ms/step\n",
            "Epoch 3/30\n",
            "189/189 - 0s - loss: 0.5952 - accuracy: 0.7206 - val_loss: 0.5884 - val_accuracy: 0.7524 - 313ms/epoch - 2ms/step\n",
            "Epoch 4/30\n",
            "189/189 - 0s - loss: 0.5607 - accuracy: 0.7540 - val_loss: 0.5544 - val_accuracy: 0.7619 - 315ms/epoch - 2ms/step\n",
            "Epoch 5/30\n",
            "189/189 - 0s - loss: 0.5234 - accuracy: 0.8011 - val_loss: 0.5175 - val_accuracy: 0.7857 - 285ms/epoch - 2ms/step\n",
            "Epoch 6/30\n",
            "189/189 - 0s - loss: 0.4840 - accuracy: 0.8312 - val_loss: 0.4784 - val_accuracy: 0.8190 - 315ms/epoch - 2ms/step\n",
            "Epoch 7/30\n",
            "189/189 - 0s - loss: 0.4439 - accuracy: 0.8561 - val_loss: 0.4390 - val_accuracy: 0.8524 - 318ms/epoch - 2ms/step\n",
            "Epoch 8/30\n",
            "189/189 - 0s - loss: 0.4045 - accuracy: 0.8810 - val_loss: 0.4008 - val_accuracy: 0.8762 - 307ms/epoch - 2ms/step\n",
            "Epoch 9/30\n",
            "189/189 - 0s - loss: 0.3670 - accuracy: 0.9032 - val_loss: 0.3645 - val_accuracy: 0.8857 - 328ms/epoch - 2ms/step\n",
            "Epoch 10/30\n",
            "189/189 - 0s - loss: 0.3319 - accuracy: 0.9138 - val_loss: 0.3318 - val_accuracy: 0.8905 - 290ms/epoch - 2ms/step\n",
            "Epoch 11/30\n",
            "189/189 - 0s - loss: 0.3003 - accuracy: 0.9270 - val_loss: 0.3017 - val_accuracy: 0.9143 - 275ms/epoch - 1ms/step\n",
            "Epoch 12/30\n",
            "189/189 - 0s - loss: 0.2723 - accuracy: 0.9365 - val_loss: 0.2759 - val_accuracy: 0.9333 - 276ms/epoch - 1ms/step\n",
            "Epoch 13/30\n",
            "189/189 - 0s - loss: 0.2480 - accuracy: 0.9460 - val_loss: 0.2537 - val_accuracy: 0.9333 - 283ms/epoch - 1ms/step\n",
            "Epoch 14/30\n",
            "189/189 - 0s - loss: 0.2270 - accuracy: 0.9503 - val_loss: 0.2337 - val_accuracy: 0.9476 - 309ms/epoch - 2ms/step\n",
            "Epoch 15/30\n",
            "189/189 - 0s - loss: 0.2083 - accuracy: 0.9587 - val_loss: 0.2153 - val_accuracy: 0.9571 - 321ms/epoch - 2ms/step\n",
            "Epoch 16/30\n",
            "189/189 - 0s - loss: 0.1906 - accuracy: 0.9593 - val_loss: 0.1989 - val_accuracy: 0.9571 - 279ms/epoch - 1ms/step\n",
            "Epoch 17/30\n",
            "189/189 - 0s - loss: 0.1758 - accuracy: 0.9693 - val_loss: 0.1851 - val_accuracy: 0.9571 - 331ms/epoch - 2ms/step\n",
            "Epoch 18/30\n",
            "189/189 - 0s - loss: 0.1632 - accuracy: 0.9709 - val_loss: 0.1732 - val_accuracy: 0.9619 - 295ms/epoch - 2ms/step\n",
            "Epoch 19/30\n",
            "189/189 - 0s - loss: 0.1522 - accuracy: 0.9788 - val_loss: 0.1633 - val_accuracy: 0.9619 - 283ms/epoch - 1ms/step\n",
            "Epoch 20/30\n",
            "189/189 - 0s - loss: 0.1426 - accuracy: 0.9757 - val_loss: 0.1535 - val_accuracy: 0.9619 - 316ms/epoch - 2ms/step\n",
            "Epoch 21/30\n",
            "189/189 - 0s - loss: 0.1343 - accuracy: 0.9788 - val_loss: 0.1455 - val_accuracy: 0.9619 - 317ms/epoch - 2ms/step\n",
            "Epoch 22/30\n",
            "189/189 - 0s - loss: 0.1269 - accuracy: 0.9799 - val_loss: 0.1386 - val_accuracy: 0.9619 - 308ms/epoch - 2ms/step\n",
            "Epoch 23/30\n",
            "189/189 - 0s - loss: 0.1205 - accuracy: 0.9836 - val_loss: 0.1317 - val_accuracy: 0.9714 - 409ms/epoch - 2ms/step\n",
            "Epoch 24/30\n",
            "189/189 - 0s - loss: 0.1143 - accuracy: 0.9847 - val_loss: 0.1270 - val_accuracy: 0.9619 - 457ms/epoch - 2ms/step\n",
            "Epoch 25/30\n",
            "189/189 - 0s - loss: 0.1094 - accuracy: 0.9831 - val_loss: 0.1213 - val_accuracy: 0.9714 - 406ms/epoch - 2ms/step\n",
            "Epoch 26/30\n",
            "189/189 - 0s - loss: 0.1042 - accuracy: 0.9836 - val_loss: 0.1160 - val_accuracy: 0.9714 - 464ms/epoch - 2ms/step\n",
            "Epoch 27/30\n",
            "189/189 - 0s - loss: 0.1002 - accuracy: 0.9873 - val_loss: 0.1120 - val_accuracy: 0.9714 - 404ms/epoch - 2ms/step\n",
            "Epoch 28/30\n",
            "189/189 - 0s - loss: 0.0961 - accuracy: 0.9862 - val_loss: 0.1079 - val_accuracy: 0.9714 - 356ms/epoch - 2ms/step\n",
            "Epoch 29/30\n",
            "189/189 - 0s - loss: 0.0925 - accuracy: 0.9862 - val_loss: 0.1040 - val_accuracy: 0.9952 - 321ms/epoch - 2ms/step\n",
            "Epoch 30/30\n",
            "189/189 - 0s - loss: 0.0890 - accuracy: 0.9905 - val_loss: 0.1010 - val_accuracy: 0.9714 - 282ms/epoch - 1ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ad0e44c0970>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generating a validation set on the fly with keras\n",
        "\n",
        "# include the 'validation_split=0.2' parameter"
      ],
      "metadata": {
        "id": "cmzshECOCdfm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test samples\n",
        "test_samples = []\n",
        "\n",
        "test_labels = []\n",
        "\n",
        "\n",
        "for i in range(200):\n",
        "  random_younger = randint(13,64)\n",
        "  test_samples.append(random_younger)\n",
        "  test_labels.append(1)\n",
        "\n",
        "  random_older = randint(65,100)\n",
        "  test_samples.append(random_older)\n",
        "  test_labels.append(0)\n"
      ],
      "metadata": {
        "id": "_e8EBXe8DchW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels = np.array(train_labels)\n",
        "test_samples = np.array(train_samples)\n",
        "\n",
        "test_labels, test_samples = shuffle(test_labels, test_samples)"
      ],
      "metadata": {
        "id": "MK2YNg2ch0iO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler(feature_range =(0,1))\n",
        "\n",
        "scaled_test_samples = scaler.fit_transform(test_samples.reshape(-1,1))"
      ],
      "metadata": {
        "id": "OKmgWJoviCb3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predicting on the test labels\n",
        "predictions = model.predict(x=scaled_test_samples, batch_size=10, verbose=0)"
      ],
      "metadata": {
        "id": "7kem5DpCiLd0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in predictions[0:10]:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jm0gSSDdik1U",
        "outputId": "433b26f6-f3b6-4558-e02e-c5905d3f322f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.9983779  0.00162221]\n",
            "[0.18888628 0.8111137 ]\n",
            "[0.9777508  0.02224925]\n",
            "[0.40961614 0.5903838 ]\n",
            "[0.9883182  0.01168181]\n",
            "[9.991651e-01 8.348269e-04]\n",
            "[0.00899162 0.9910083 ]\n",
            "[0.18888628 0.8111137 ]\n",
            "[0.01133345 0.98866653]\n",
            "[0.9777508  0.02224925]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "round_pred = np.around(predictions, 3)"
      ],
      "metadata": {
        "id": "CGWPLFnjkiJv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "round_pred[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpT1v1uVoP7M",
        "outputId": "1ab844f0-78c7-495e-b22a-7f4a60538608"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.998, 0.002],\n",
              "       [0.189, 0.811],\n",
              "       [0.978, 0.022],\n",
              "       [0.41 , 0.59 ],\n",
              "       [0.988, 0.012],\n",
              "       [0.999, 0.001],\n",
              "       [0.009, 0.991],\n",
              "       [0.189, 0.811],\n",
              "       [0.011, 0.989],\n",
              "       [0.978, 0.022]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = np.argmax(predictions, axis=-1)"
      ],
      "metadata": {
        "id": "M845MzHZuNM0"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred[0:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgU0veoSwaVQ",
        "outputId": "038af8be-ac49-4f50-ee41-346fc8ee4d6d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: produce a confusion matrix for test_labels and pred using seaborn to plot them and scikit learn confusion matrix\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(test_labels, pred)\n",
        "\n",
        "ax = plt.subplot()\n",
        "sns.heatmap(cm, annot=True, ax=ax)  # Annotate cells with values\n",
        "\n",
        "ax.set_xlabel('Predicted labels')\n",
        "ax.set_ylabel('True labels')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(['YES', 'NO'])\n",
        "ax.yaxis.set_ticklabels(['YES', 'NO'])\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "WC4lm7ufwdKA",
        "outputId": "7c821424-2fc4-47e1-eb87-d6bd842e43eb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saving and loading a sequantial model"
      ],
      "metadata": {
        "id": "WvvyZEMLxuAh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}